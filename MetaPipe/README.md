# MetaPipe (metap)

**Adaptive Metaâ€‘Orchestration for Taskâ€‘Aware LLM Pipeline Routing**

> Paperâ€‘aligned module for: **â€œMetaPipe: Adaptive Metaâ€‘Orchestration for Taskâ€‘Aware LLM Pipeline Routing.â€**

---

## ğŸ’¡ What is MetaPipe?

MetaPipe learns **which model to use for which pipeline role** (retrieval, synthesis, judging, OCR, fusion, etc.) **given the task context and constraints** (quality, latency, cost). Instead of static assignments, MetaPipe uses a **metaâ€‘controller** that observes features of the task and **routes** each stage to the best LLM/tool. It can **reconfigure midâ€‘run** when confidence drops or inputs degrade.

Works across your three exemplar pipelines:

* Agentic RAG (deep document analysis)
* Multiâ€‘Agent Scientific Research
* Vision + Text (scanned/handwritten forms)

---

## âœ¨ Core Capabilities

* **Task & Context Embeddings** â€” encode signals like query/domain, length, OCR quality, retrieval entropy, past success/latency/cost.
* **Metaâ€‘Controller (Policy)** â€” learns a routing policy over a *candidate set* of LLMs/tools per role (e.g., Fastâ€‘LLM vs. Strongâ€‘LLM).
* **Dynamic Reconfiguration** â€” midâ€‘pipeline switching based on uncertainty or budget remaining ("escalate on fail").
* **Multiâ€‘Objective Optimization** â€” trade off quality, latency, and dollars; supports hard constraints (e.g., max 1.5s).
* **Plugâ€‘andâ€‘Play** â€” wrap existing pipelines with thin adapters; no heavy refactors.

---

## ğŸ§± Architecture

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Candidates per role â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Query/Input â†’  Role 1: {LLM_A, LLM_B, Tool_X}  â†â”€â”
               Role 2: {LLM_C, LLM_D}           â”€â”¼â”€>  Metaâ€‘Controller Ï€( a | Ï•(task, state) )
               Role 3: {Judge1, Judge2, None}   â†â”˜
                                 â”‚ actions (model picks)
                                 â–¼
                         Executed Pipeline
                                 â”‚ feedback
                                 â–¼
                           Experience Buffer â†’ Policy Update
```

**Packages**

* `metap.features` â€” builders for task/context embeddings (static + online signals).
* `metap.policy` â€” metaâ€‘controller implementations: heuristic, contextual bandit, RL, or supervised matching network.
* `metap.runtime` â€” router + executors + escalation logic; budget/SLAs.
* `metap.evals` â€” multiâ€‘objective metrics, regret curves, Pareto plots, success\@budget.
* `metap.adapters` â€” wrappers for RAG, multiâ€‘agent, and visionâ€‘text pipelines.

---

## ğŸš€ Quickstart

### 1) Install

```bash
pip install -e ./metap
```

### 2) Define candidates per role

```python
CANDIDATES = {
  "retrieve": [bm25, hybrid, dense],
  "synthesize": [fast_llm, strong_llm],
  "judge": [none, gpt_judge, two_stage_critique],
}
```

### 3) Build features & policy

```python
from metap.features import build_features
from metap.policy import ContextualBandit
from metap.runtime import Router

policy = ContextualBandit(candidates=CANDIDATES)
router = Router(policy=policy)

ctx = build_features(query=q, retrieved_k=10, domain="legal", budget_usd=0.01)
plan = router.plan(ctx)         # e.g., {retrieve: hybrid, synthesize: fast_llm, judge: gpt_judge}
result = router.execute(plan, query=q)
```

### 4) Online learning (bandit/RL)

```python
reward = result.metrics["quality"] - 0.1*result.metrics["cost_usd"]
policy.update(context=ctx, action=plan, reward=reward)
```

---

## âš™ï¸ Configuration

YAML example:

```yaml
experiment: metap_rag_v1
pipeline: rag
budget_usd: 0.02
sla_ms: 1800
candidates:
  retrieve: [bm25, dense, hybrid]
  synthesize: [fast_llm, strong_llm]
  judge: [none, gpt_judge]
policy:
  type: contextual_bandit   # {heuristic, contextual_bandit, rl, matcher}
  features: [len_query, domain_id, retriever_entropy, ocr_quality, hist_success, hist_cost]
reconfig:
  escalate_on_low_conf: true
  escalate_threshold: 0.35
logging:
  logdir: output/metap
```

---

## ğŸ“¦ Data & Logging

```
output/
â””â”€â”€ metap/
    â”œâ”€â”€ plans/      # chosen actions per role
    â”œâ”€â”€ runs/       # execution traces (compatible with xpipe)
    â”œâ”€â”€ policy/     # snapshots of learned policy weights
    â””â”€â”€ evals/      # CSVs: quality, latency, cost, regret
```

MetaPipe is **xpipeâ€‘compatible**: you can record `Trace`/`MetricLog` during execution for unified dashboards.

---

## ğŸ“Š Evaluation

* **Quality**: task success / correctness / VQAâ€‘style scoring.
* **Latency & Cost**: perâ€‘stage + endâ€‘toâ€‘end; budget adherence.
* **Regret**: vs. best static policy and oracle perâ€‘role picks.
* **Pareto**: qualityâ€“cost and qualityâ€“latency frontiers.
* **Escalation Efficacy**: success lift vs. extra spend.

Reproduce baseline vs. MetaPipe:

```bash
python scripts/run_metap.py \
  --pipeline rag --dataset docs-mini \
  --policy contextual_bandit \
  --out output/metap
```

---

## ğŸ”¬ Policies Included

* **Heuristic**: handâ€‘crafted if/else on features (fast baseline).
* **Contextual Bandit**: LinUCB/Thompson sampling with engineered features.
* **RL Controller**: (optional) actorâ€‘critic over multiâ€‘step routing with budget.
* **Matcher**: supervised model from offline logs mapping contexts â†’ best actions.

You can start with **Heuristic** or **Bandit** (minimal code), then graduate to **RL**.

---

## ğŸ§© Adapters

* `metap.adapters.rag` â€” wraps retriever/synthesizer/judge.
* `metap.adapters.multi_agent` â€” routes agent roles and escalation.
* `metap.adapters.vision_text` â€” gates OCR model choice and fusion.

Adapters expose a common interface so policies stay pipelineâ€‘agnostic.

---

## ğŸ›¡ï¸ Constraints & Safety

* **Budgets**: hard and soft caps (drop/abstain when exceeded).
* **Confidenceâ€‘aware Routing**: backoff/abstain, or escalate.
* **Privacy**: redact traces; local logs by default.

---

## ğŸ—ºï¸ Roadmap

* Learned feature encoders from raw traces (sequence models over spans)
* Multiâ€‘objective RL with constraint handling (Lagrangian / CPO)
* Crossâ€‘pipeline transfer (train on RAG, reuse on visionâ€‘text)
* Policy distillation into tiny heuristics for edge deployment

---

## ğŸ“š Citation (paper preprint)

```
@inproceedings{mesabo2025metapipe,
  title     = {MetaPipe: Adaptive Meta-Orchestration for Task-Aware LLM Pipeline Routing},
  author    = {Messou, Franck J. A. and Collaborators},
  booktitle = {Proc. of <Venue>},
  year      = {2025}
}
```

---

## ğŸ“ License

Follows repository license (**BSDâ€‘3â€‘Clause**).

---

## ğŸ™Œ Acknowledgements

Built on top of the projectâ€™s RAG, multiâ€‘agent, and visionâ€‘text pipelines. Thanks to contributors for early discussions on routing, bandits, and escalation policies.
