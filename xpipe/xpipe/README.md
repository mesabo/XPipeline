# X-Pipe (xpipe)

**An Explainable Evaluation Framework for Multiâ€‘Stage LLM Pipelines**

> Paper-aligned module for the project: **â€œXâ€‘Pipe: An Explainable Evaluation Framework for Multiâ€‘Stage LLM Pipelines.â€**

---

## ğŸ” What is Xâ€‘Pipe?

Xâ€‘Pipe makes complex LLM pipelines **transparent, debuggable, and comparable**. It instruments every stage (RAG, multiâ€‘agent reasoning, visionâ€‘text OCR/fusion, safety checks, judges, etc.), logs *why* decisions were made, and visualizes *where* errors originate. It also supports **causal attribution** and **ablations** to quantify each componentâ€™s contribution to the final outcome.

Xâ€‘Pipe plugs into your existing notebooks/modules:

* `01_agentic_RAG.ipynb` (agentic RAG)
* `02_multi_agent_system.ipynb` (research multiâ€‘agent)
* `03_vision_reasoning.ipynb` (vision + reasoning for scanned forms)

---

## âœ¨ Core Capabilities

* **Agentic Trace Logging** â€” capture prompts, intermediate outputs, tool/RAG calls, costs, latencies, and confidence scores per stage.
* **Causal Attribution** â€” attribute success/failure to specific components (router, retriever, synthesizer, judge, OCR, fusion) using perturbation- and proxyâ€‘based methods.
* **Ablation & Whatâ€‘If Analysis** â€” disable/replace agents and quantify performance deltas ("map of responsibility").
* **Unified Metrics** â€” relevance, faithfulness, answer correctness, latency, cost, and calibration (selective risk / abstention).
* **Interactive Dashboard** â€” inspect runs, compare pipelines, trace error propagation.

---

## ğŸ§± Architecture

```
request â”€â–¶ Stage 1 (Retriever/Router) â”€â–¶ Stage 2 (Reasoner/Synthesizer) â”€â–¶ Stage 3 (Judge/Safety)
          â”‚                              â”‚                                  â”‚
          â””â”€â–¶ xpipe.trace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â–¶ xpipe.metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â–¶ xpipe.attribution
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ xpipe.dashboard
```

**Packages**

* `xpipe.trace` â€” structured events, spans, artifacts (prompts, contexts, images, tool calls).
* `xpipe.metrics` â€” task metrics (accuracy, BLEU/ROUGE, faithfulness, calibration), runtime/cost.
* `xpipe.attribution` â€” componentâ€‘level responsibility estimates (perturbation, swapâ€‘outs, SHAPâ€‘style proxies where applicable).
* `xpipe.dashboard` â€” local UI (Streamlit/Gradio) for filtering runs, drilling into stages, and comparing variants.
* `xpipe.runners` â€” thin adapters for your existing pipelines (RAG, multiâ€‘agent, visionâ€‘text).

---

## ğŸš€ Quickstart

### 1) Install

```bash
# from the repo root
pip install -e .
# or, module-only
pip install -e ./xpipe
```

### 2) Instrument your pipeline

```python
from xpipe.trace import Trace
from xpipe.metrics import MetricLog

trace = Trace(experiment="rag_v1", run_tags={"dataset":"docs-mini"})
metrics = MetricLog()

with trace.span("retrieve") as sp:
    ctx = retriever(query)
    sp.log({"k": len(ctx), "latency_ms": 12})

with trace.span("synthesize") as sp:
    answer = llm_synth(query, ctx)
    sp.log({"prompt_tokens": 356, "completion_tokens": 221})

metrics.add(
    relevance=score_relevance(answer, ctx),
    faithfulness=score_faithfulness(answer, ctx),
    latency_ms=trace.wall_time_ms,
    cost_usd=estimate_cost(trace.token_usage),
)

trace.save(); metrics.save()
```

### 3) Run ablations

```python
from xpipe.attribution import ablate

results = ablate(
    pipeline_fn=my_pipeline,              # callable(query) -> answer
    factors={
        "retriever": ["bm25", "hybrid", "dense"],
        "judge": ["none", "gpt-judge", "critique-2stage"],
    },
    dataset=queries[:100]
)
results.to_csv("./output/xpipe_ablations.csv")
```

### 4) Launch the dashboard

```bash
python -m xpipe.dashboard --logdir ./output/xpipe_logs
```

---

## ğŸ“¦ Data & Logging Layout

```
output/
â””â”€â”€ xpipe/
    â”œâ”€â”€ runs/
    â”‚   â””â”€â”€ 2025-08-24_13-05-22_rag_v1.jsonl   # unified trace (events/spans/artifacts)
    â”œâ”€â”€ metrics/
    â”‚   â””â”€â”€ rag_v1_metrics.csv                 # per-run metrics (task + system)
    â”œâ”€â”€ ablations/
    â”‚   â””â”€â”€ rag_v1_ablate.csv                  # grid/what-if outcomes
    â””â”€â”€ figs/
        â”œâ”€â”€ trace_graph.png
        â””â”€â”€ calibration_plot.png
```

---

## ğŸ“Š Metrics (Builtâ€‘ins)

* **Task Quality**: exact/partial match, ROUGEâ€‘L, BLEU, answerable F1, VQAâ€‘style scores.
* **Groundedness**: faithfulness (evidence overlap), attribution of citations, hallucination rate.
* **System**: total latency, perâ€‘stage latency, token counts, \$\$ cost, GPU/CPU utilization (optional).
* **Calibration**: confidence vs. accuracy (ECE), selective prediction (riskâ€‘coverage).

> Plug your own scorers via the `xpipe.metrics.Registry`.

---

## ğŸ§ª Experiments

* **Pipelines**: agenticâ€‘RAG, scientific multiâ€‘agent, visionâ€‘text OCR/fusion.
* **Comparisons**: fixed vs. adaptive routing, singleâ€‘judge vs. multiâ€‘judge, single vs. hybrid retrieval.
* **Stress Tests**: noisy OCR, ambiguous instructions, longâ€‘context documents, crossâ€‘domain generalization.

Reproduce with:

```bash
python scripts/run_experiment.py \
  --pipeline rag \
  --dataset docs-mini \
  --variants retriever=hybrid judge=gpt-judge \
  --out output/xpipe
```

---

## âš™ï¸ Configuration

YAML/CLI for reproducibility. Example:

```yaml
experiment: rag_v1
logdir: output/xpipe
pipeline: rag
variants:
  retriever: [bm25, dense, hybrid]
  judge: [none, gpt-judge]
metrics:
  - relevance
  - faithfulness
  - latency_ms
  - cost_usd
```

---

## ğŸ–¥ï¸ Dashboard Features

* Run table with filters (pipeline, dataset, date, variants)
* Span graph (timeline per stage, with drillâ€‘down)
* Error propagation view (stage â†’ stage)
* Calibration & selective prediction plots
* Compare runs sideâ€‘byâ€‘side (diff tokens/cost/latency/quality)

---

## ğŸ§© Adapters (Runners)

* `xpipe.runners.rag` â€” instrument retrieval, reranking, synthesis, judging.
* `xpipe.runners.multi_agent` â€” capture agent messages, roles, tool calls.
* `xpipe.runners.vision_text` â€” log OCR outputs, layout parsing, fusion steps.

Each runner exposes a minimal interface to wrap an existing pipeline without refactoring core logic.

---

## ğŸ›¡ï¸ Reliability & Privacy

* Optional redaction of PII in traces.
* Localâ€‘only logging by default; remote storage requires explicit optâ€‘in.
* Deterministic seeds for evaluators and ablations.

---

## ğŸ—ºï¸ Roadmap

* Modelâ€‘agnostic calibration (temperature scaling + conformal risk control)
* Automated failure clustering (trace â†’ failure taxonomy)
* Crossâ€‘run statistical tests and confidence intervals
* Export to W\&B / MLflow; Grafana datasource for system metrics

---

## ğŸ“š Citation (paper preprint)

```
@inproceedings{mesabo2025xpipe,
  title     = {X-Pipe: An Explainable Evaluation Framework for Multi-Stage LLM Pipelines},
  author    = {Messou, Franck J. A. and Collaborators},
  booktitle = {Proc. of <Venue>},
  year      = {2025}
}
```

---

## ğŸ“ License

This module follows the repository license (**BSDâ€‘3â€‘Clause**).

---

## ğŸ™Œ Acknowledgements

Built on top of the projectâ€™s three exemplar pipelines (Agentic RAG, Multiâ€‘Agent Research, Visionâ€‘Text Reasoning). Thanks to contributors and reviewers for feedback on instrumentation, metrics, and dashboard design.
