# X-Pipe â€” An Explainable Evaluation Framework for Multi-Stage LLM Pipelines

Paper-aligned module for â€œX-Pipe: An Explainable Evaluation Framework for Multi-Stage LLM Pipelines.â€

## ğŸ” Overview
X-Pipe instruments multi-stage LLM pipelines (RAG, multi-agent, vision-text, judges) to make them transparent, debuggable, and comparable. It captures per-stage decision rationales, logs traces and metrics, and supports causal attribution (ablations) to identify error sources.

Key features
- Causal attribution via ablations
- Thin adapters to integrate into existing workflows
- Tracing of spans, events, artifacts, and token usage
- Metric logging (relevance, faithfulness, latency, cost)
- Example notebooks for reproducible demos

Notebooks
- `01_agentic_RAG.ipynb` â€” Agentic RAG pipeline
- `02_multi_agent_system.ipynb` â€” Multi-agent research workflows
- `03_vision_reasoning.ipynb` â€” OCR + reasoning over scanned forms

---

## âœ… Getting Started

### Environment
Choose one Conda environment:

CPU-only
```bash
conda env create -f env_llms_cpu.yml
conda activate llms
```

GPU (CUDA 12.1 via Conda)
```bash
conda env create -f env_llms_gpu.yml
conda activate llms
```

Tip for offline servers
```bash
export TRANSFORMERS_OFFLINE=1
export HF_HOME=/shared/hf_cache
```

### Quickstart

Single run (RAG demo)
```bash
python xpipe/main.py --config xpipe/configs/experiment_rag.yaml
```

Outputs (examples)
- `output/xpipe/runs/<stamp>_xpipe_rag_*.jsonl`  
- `output/xpipe/runs/<stamp>_xpipe_rag_*.pretty.json` (human readable)  
- `output/xpipe/metrics/xpipe_rag_*.csv`

Sweep multiple LLMs
```bash
# All synthesizer models
./xpipe/scripts/run_all_llms.sh

# Hugging Face models only
./xpipe/scripts/run_all_llms.sh synth "hf/"

# Sweep judges only
./xpipe/scripts/run_all_llms.sh judge

# Full grid (synth Ã— judge, limit to 8 combos)
./xpipe/scripts/run_all_llms.sh grid "" "" 8
```

The script reports: `[sweep] Prepared N configs in output/xpipe/tmp_configs`.

---

## ğŸ§± Architecture

Pipeline (high level)
```
request â†’ Stage 1 (Retriever / Router)
        â†’ Stage 2 (Reasoner / Synthesizer)
        â†’ Stage 3 (Judge / Safety)
```

Core modules
- `xpipe.trace` â€” spans, events, artifacts, token usage
- `xpipe.metrics` â€” metric registry, CSV export
- `xpipe.attribution` â€” ablation and responsibility mapping
- `xpipe.runners` â€” demo runners (RAG, multi-agent, vision-text)
- `xpipe.llm_backends` â€” HF + optional Ollama dispatch

Paper claims implemented
- Explainable tracing per stage (spans, artifacts, token counts)
- Holistic metrics (relevance, faithfulness, latency, cost)
- Causal attribution via ablations
- Flexible model selection per stage via config

---

## ğŸ“¦ Directory Structure
```
xpipe/
â”œâ”€â”€ main.py                    # Entry point (RAG demo + trace/metrics)
â”œâ”€â”€ xpipe/                     # Core library package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trace.py               # Spans, events, JSONL writer
â”‚   â”œâ”€â”€ metrics.py             # MetricLog (CSV output)
â”‚   â”œâ”€â”€ attribution.py         # Ablation/grid utilities
â”‚   â”œâ”€â”€ llm_backends.py        # HF + optional Ollama dispatch
â”‚   â””â”€â”€ runners/               # Adapters for demos
â”‚       â”œâ”€â”€ rag.py
â”‚       â”œâ”€â”€ multi_agent.py
â”‚       â””â”€â”€ vision_text.py
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ experiment_rag.yaml    # Pipeline, corpus, queries, stage â†’ model
â”‚   â””â”€â”€ models.yaml            # Model handles (HF, optional Ollama)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_all_llms.sh        # Sweeps models, prepares configs
â”‚   â””â”€â”€ run_xpipe.sbatch       # Optional SLURM launcher
â””â”€â”€ output/xpipe/
    â”œâ”€â”€ runs/                  # JSONL traces + pretty JSON
    â”œâ”€â”€ metrics/               # CSV metrics
    â”œâ”€â”€ ablations/             # Ablation grids
    â””â”€â”€ figs/                  # Plots
```

---

## âš™ï¸ Configuration

Example `experiment_rag.yaml` (excerpt)
```yaml
name: xpipe_rag_free
pipeline: rag
logdir: output/xpipe
retriever:
  top_k: 3
llms:
  synthesize:
    model: hf/gpt2
    params:
      max_new_tokens: 200
      temperature: 0.2
  judge:
    model: heuristic
corpus:
  - {id: d1, text: "Paris is the capital of France."}
queries:
  - {id: q1, text: "What is the capital of France?"}
```

Example `models.yaml` (excerpt)
```yaml
hf/distilgpt2:
  backend: hf
  id: distilgpt2
  params: {max_new_tokens: 160, temperature: 0.0}
hf/Qwen2.5-0.5B-Instruct:
  backend: hf
  id: Qwen/Qwen2.5-0.5B-Instruct
  params: {max_new_tokens: 200, temperature: 0.2}
```

Notes
- Stages (e.g., `synthesize`, `judge`) can independently select models from `models.yaml`.
- Stage-specific params override defaults from `models.yaml`.
- `models_path` in `experiment_rag.yaml` can point to custom model files.

---

## ğŸ”¬ Outputs

Traces (`output/xpipe/runs/*.jsonl`, `*.pretty.json`) contain:
- Per-stage spans (start/log/end)
- Artifacts (prompts, contexts)
- Latency and token usage

Example trace schema
```json
{
  "experiment": "xpipe_rag_free",
  "run_id": "4766ed64",
  "run_tags": {"pipeline": "rag"},
  "started_ts_ms": 172449...,
  "wall_time_ms": 1543,
  "token_usage": {"prompt": 345, "completion": 214},
  "events": [...]
}
```

Metrics (`output/xpipe/metrics/*.csv`)
- Columns: pipeline, item (query ID), relevance, faithfulness, latency_ms, cost_usd
- Custom metrics can be added via `MetricLog.add()`

---

## ğŸ§ª Ablations

Run ablations to analyze component contributions:
```python
from xpipe.attribution import ablate

def my_pipeline(item, retriever, judge):
    # Pipeline logic
    return {"scores": ...}

df = ablate(
    pipeline_fn=my_pipeline,
    dataset=["q1", "q2", "q3"],
    factors={"retriever": ["bm25", "dense", "hybrid"],
             "judge": ["heuristic", "hf/gpt2"]}
)
df.to_csv("output/xpipe/ablations/ablate.csv")
```

---

## ğŸ§© Notebooks & Examples
Reuse core components in notebooks:
```python
from xpipe.trace import Trace
from xpipe.metrics import MetricLog
from xpipe.llm_backends import call_llm
```

---

## ğŸ§° Troubleshooting

- Ollama connection refused: restrict to Hugging Face models:
  ```bash
  ./xpipe/scripts/run_all_llms.sh synth "hf/"
  ```
- GLIBCXX / pandas errors: use provided Conda envs (`env_llms_cpu.yml`, `env_llms_gpu.yml`).
- Out of VRAM: prefer `hf/distilgpt2` or `hf/gpt2` for lower memory usage.
- Offline HF weights: set `TRANSFORMERS_OFFLINE=1` and `HF_HOME=/path/to/shared/hf_cache`.

---

## ğŸ—ºï¸ Roadmap
- Model-agnostic calibration (temperature scaling, conformal control)
- Failure clustering from traces
- Cross-run significance tests / confidence intervals
- Integration with W&B, MLflow, Grafana

---

## ğŸ›¡ï¸ Reliability & Privacy
- Local-only logging by default
- Optional PII redaction in traces
- Deterministic seeds for evaluators and ablations

---

## ğŸ“š Citation
```
@inproceedings{mesabo2025xpipe,
  title     = {X-Pipe: An Explainable Evaluation Framework for Multi-Stage LLM Pipelines},
  author    = {Messou, Franck J. A. and Collaborators},
  booktitle = {Proc. of <Conference to be continued>},
  year      = {2025}
}
```

---

## ğŸ™Œ Acknowledgements
Built on three exemplar pipelines. Thanks to contributors and reviewers for guidance on instrumentation and metrics.

## ğŸ“ License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.