# CoCoPipe (cocop)

**Crossâ€‘Modal Coherence in Multiâ€‘Agent Visionâ€‘andâ€‘Text Pipelines**

> Paperâ€‘aligned module for: **â€œCoCoPipe: Crossâ€‘Modal Coherence in Multiâ€‘Agent Visionâ€‘andâ€‘Text LLM Pipelines.â€**

---

## ğŸ’¡ What is CoCoPipe?

CoCoPipe studies **how well multiple agents across text and vision modalities stay consistent** when working together in complex pipelines. It provides **metrics, benchmarks, and adapters** to measure and improve *coherence* between OCR outputs, reasoning steps, synthesis agents, and final answers.

Where `xpipe` focuses on explainability and `metap` on adaptive routing, `cocop` zeroes in on **crossâ€‘modal consistency** â€” making sure that what the vision model reads is what the text model reasons over and what the synthesis model outputs.

---

## âœ¨ Core Capabilities

* **Crossâ€‘Modal Task Partitioning** â€” define standardized roles across modalities (e.g., OCR, fusion, synthesizer, evaluator).
* **Coherence Metrics** â€” quantitative scores for semantic/structural alignment across agents (e.g., OCR vs. reasoning vs. synthesis).
* **Stressâ€‘Testing** â€” evaluate pipelines on noisy, adversarial, or degraded visual inputs (handwriting, lowâ€‘res scans).
* **Feedback Loops** â€” optional mechanisms for agents to send *coherence feedback* to upstream peers.
* **Benchmarks & Datasets** â€” curated forms and documents with varying quality to test robustness.

---

## ğŸ§± Architecture

```
   Image/Form â†’ [ OCR Agent ] â†’ raw JSON/text â†’ [ Fusion Agent ]
                                   â”‚
                  question/context â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [ Reasoning Agent ]
                                   â”‚
                                   â–¼
                               [ Synthesizer ]
                                   â”‚
                                   â–¼
                              [ Evaluator (Judge) ]
                                   â”‚
                               Coherence Score(s)
```

**Packages**

* `cocop.ocr` â€” wrappers for vision models (OCR agents).
* `cocop.fusion` â€” combine raw OCR text with structured schemas.
* `cocop.reason` â€” reasoning/refinement agents.
* `cocop.synth` â€” synthesis agents.
* `cocop.eval` â€” coherence metrics + evaluation harness.
* `cocop.datasets` â€” noisy/clean benchmark sets.

---

## ğŸš€ Quickstart

### 1) Install

```bash
pip install -e ./cocop
```

### 2) Minimal Run

```python
from cocop import Pipeline, datasets

pipe = Pipeline(
  ocr_model="gemma-vision-27b",
  reason_model="qwen3-14b",
  synth_model="llama-3.3-70b",
  eval_model="deepseek-v3"
)

form = datasets.load("insurance_form_noisy")
result = pipe.run(image=form.image, question="What is the applicantâ€™s email?")

print(result.answer)
print(result.metrics["coherence_score"])
```

---

## âš™ï¸ Configuration

```yaml
experiment: cocop_insurance_v1
pipeline: vision_text
ocr_model: gemma-vision-27b
reason_model: qwen3-14b
synth_model: llama-3.3-70b
judge_model: deepseek-v3
metrics:
  coherence: true
  faithfulness: true
  retrieval: false
stress_test:
  noise_levels: [clean, medium, heavy]
logging:
  logdir: output/cocop
```

---

## ğŸ“¦ Data & Logging

```
output/
â””â”€â”€ cocop/
    â”œâ”€â”€ runs/        # execution traces
    â”œâ”€â”€ metrics/     # coherence/relevance/faithfulness scores (CSV)
    â”œâ”€â”€ samples/     # perâ€‘query annotated inputs/outputs
    â””â”€â”€ evals/       # summary reports
```

---

## ğŸ“Š Coherence Metrics

* **Lexical Overlap**: do OCR terms survive to synthesis?
* **Entity Consistency**: are key entities (names, IDs, dates) consistent across steps?
* **Structural Alignment**: schema conformity (e.g., Pydantic contract).
* **Crossâ€‘Agent Agreement**: evaluator checks if reasoning contradicts OCR.
* **Robustness**: drop in coherence under noise/degradation.

---

## ğŸ”¬ Evaluation

Run builtâ€‘in evaluation suites:

```bash
python scripts/run_cocop.py \
  --dataset insurance_forms --noise heavy \
  --out output/cocop/evals
```

Generates CSVs + plots with coherence breakdown per role + per noise level.

---

## ğŸ§© Adapters

* `cocop.adapters.rag` â€” test RAG coherence when OCR text is used as context.
* `cocop.adapters.multi_agent` â€” apply coherence metrics to multiâ€‘agent reasoning traces.
* `cocop.adapters.vision_text` â€” default adapter for OCR â†’ reasoning â†’ synthesis.

---

## ğŸ›¡ï¸ Constraints & Safety

* Noisy input handling (handwriting, lowâ€‘res, artifacts).
* Explicit flagging of uncertain/ambiguous fields for **human review**.
* Privacy: local logging; redact sensitive fields.

---

## ğŸ—ºï¸ Roadmap

* Learnable coherence metric (contrastive embeddings between OCR & synthesis).
* Active learning with *coherenceâ€‘based feedback*.
* Crossâ€‘pipeline generalization (train coherence models on forms, apply to RAG).
* Plugâ€‘in visualization dashboard (like xpipe) for sideâ€‘byâ€‘side OCR vs. synthesis.

---

## ğŸ“š Citation (paper preprint)

```
@inproceedings{mesabo2025cocopipe,
  title     = {CoCoPipe: Cross-Modal Coherence in Multi-Agent Vision-and-Text LLM Pipelines},
  author    = {Messou, Franck J. A. and Collaborators},
  booktitle = {Proc. of <Venue>},
  year      = {2025}
}
```

---

## ğŸ“ License

This project uses the **MIT License** (see `LICENSE`).

---

## ğŸ™Œ Acknowledgements

Built on top of the Agentic RAG, Multiâ€‘Agent, and Vision pipelines in this repo. Special thanks to early testers of noisy document OCR and crossâ€‘modal evaluation.
